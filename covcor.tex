% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{yfonts}
\usepackage{bm}


\newtcolorbox{greybox}{
  colback=white,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
\newcommand{\sectionbreak}{\clearpage}

 
\newcommand{\ds}[4]{\sum_{{#1}=1}^{#3}\sum_{{#2}=1}^{#4}}
\newcommand{\us}[3]{\mathop{\sum\sum}_{1\leq{#2}<{#1}\leq{#3}}}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\amin}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\amax}[1]{\mathop{\text{argmax}}_{#1}}

\newcommand{\ci}{\perp\!\!\!\perp}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newcommand{\eps}{\epsilon}
\newcommand{\lbd}{\lambda}
\newcommand{\alp}{\alpha}
\newcommand{\df}{=:}
\newcommand{\am}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\ls}[2]{\mathop{\sum\sum}_{#1}^{#2}}
\newcommand{\ijs}{\mathop{\sum\sum}_{1\leq i<j\leq n}}
\newcommand{\jis}{\mathop{\sum\sum}_{1\leq j<i\leq n}}
\newcommand{\sij}{\sum_{i=1}^n\sum_{j=1}^n}
	
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={The Covariance of Covariances},
  pdfauthor={Jan de Leeuw},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{The Covariance of Covariances}
\author{Jan de Leeuw}
\date{March 9, 2025}

\begin{document}
\maketitle
\begin{abstract}
We derive an exact formula for the covariance of two sample covariances
using discrete variable calculations. From that expression we derive
large sample approximations of the covariance of two sample covariances
and the covariance of two sample correlations, i.e.~of the dispersion
matrix of the asymptotic normal distribution of the covariances or
correlations.
\end{abstract}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\sectionbreak

\section*{Note}\label{note}
\addcontentsline{toc}{section}{Note}

This is a working manuscript which will be expanded/updated frequently.
All suggestions for improvement are welcome. All qmd, tex, html, pdf, R,
and C files are in the public domain. Attribution will be appreciated,
but is not required. The files can be found at
\url{https://github.com/deleeuw/covcor}

\section*{Notation}\label{notation}
\addcontentsline{toc}{section}{Notation}

\begin{itemize}
\tightlist
\item
  We use the ``Dutch Convention'' of underlining random variables
  (Hemelrijk (\citeproc{ref-hemelrijk_66}{1966})).
\item
  The symbol \(:=\) is used for definitions.
\item
  A sequence \(x_n\) is \(o(n^{-r})\) if \(n^rx_n\) converges to zero.
\item
  A sequence \(\ul{x}_n\) of random variables is \(o_p(n^{-r})\) if
  \(n^r\ul{x}_n\) converges to zero in probability (Mann and Wald
  (\citeproc{ref-mann_wald_43}{1943})).
\item
  The Kronecker delta \(\delta^{i_1\cdots i_r}\) is one if all
  superscripts are equal, and zero otherwise.
\item
  If \(\ul{x}\) is a random vector, then the raw moment
  \(\mu_{i_1\cdots i_r}\) is the expected value of the product
  \(\ul{x}_{i_1}\times\cdots\times\ul{x}_{i_r}\). In the same way, the
  central moment \(\sigma_{i_1\cdots i_r}\) is the expected value of the
  product
  \((\ul{x}_{i_1}-\mu_{i_1})\times\cdots\times(\ul{x}_{i_r}-\mu_{i_r})\).
\item
  If \(f\) is a real valued function on \(\mathbb{R}^n\), differentiable
  in a neighborhood of \(x=(x_1,\cdots,x_n)\), then
  \(\mathcal{D}_if(x)\) is the partial derivative of \(f\) with respect
  to the \(i\)-th argument at \(x\). Higher order derivatives are
  \(\mathcal{D}_{ij}f(x)\), etc.
\end{itemize}

\sectionbreak

\section{Data}\label{data}

In this paper the data are \(n\) observations on \(m\) discrete
numerical variables. Thus the data can be represented by a numerical
matrix \(X\) with \(n\) rows and \(m\) columns. In R parlance \(X\) is a
data frame (R Core Team (\citeproc{ref-r_core_team_24}{2024})). In this
paper we prefer a different representation.

Say variable \(i\) has \(v_i\) \emph{categories} or \emph{levels}. Thus
there are \(M:=\smash{\prod_{i=1}^m}v_i\) \emph{profiles} or
\emph{cells}, which are the possible outcomes of a measurement of the
\(m\) variables. The \(n\) observations produce a vector
\(N:=(n_1,\cdots,n_M)\) with \emph{profile frequencies}, whose elements
add up to \(n\). The data is now coded as a vector of length \(M\) with
the profile frequencies (see Gifi (\citeproc{ref-gifi_B_90}{1990}),
Chapter 2). In any reasonable data set the number of profiles \(M\) will
be much larger than the number of observations \(n\), and consequently
many of the profile frequencies \(n_\alpha\) will be zero.

Our statistical model for these data is that \(n\) is a realization of a
multinomial random vector \(\ul{n}\), where the multinomial has
parameters \(\pi\) and \(n\). Thus \(\ul{n}\) is the outcome of \(n\)
independent trials in each of which which we sample profile \(\alpha\)
with probability \(\pi_\alpha\).

Just to be clear: in the rest of the paper subscripts
\(1\leq i,j,k,l\leq m\) refer to variables, subscripts
\(1\leq \alpha,\beta,\gamma,\delta\leq M\) refer to profiles.

\sectionbreak

\section{Moments and Product Moments}\label{moments-and-product-moments}

The covariance between variables \(i\) and \(j\) \begin{equation}
\ul{c}_{ij}=\frac{1}{n}\sum_{\gamma=1}^M\ul{n}_\gamma x_{\gamma i}x_{\gamma j}-\frac{1}{n^2}\sum_{\alpha=1}^M\sum_{\beta=1}^M\ul{n}_\alpha\ul{n}_\beta x_{\alpha i}x_{\beta j}.\label{eq-covfreq}
\end{equation} is a quadratic function of multinomial frequencies. Thus
the product of two covariances \begin{align}
\ul{c}_{ij}\ul{c}_{kl}=&\frac{1}{n^2}\sum_{\alpha=1}^m\sum_{\beta=1}^m\ul{n}_\alpha \ul{n}_\beta x_{\alpha i}x_{\alpha j}x_{\beta k}x_{\beta l}\notag\\
-&\frac{1}{n^3}\sum_{\gamma=1}^m\sum_{\alpha=1}^m\sum_{\beta=1}^m\ul{n}_\alpha \ul{n}_\beta\ul{n}_\gamma\{x_{\alpha k}x_{\beta l}x_{\gamma i}x_{\gamma j}
+x_{\alpha i}x_{\beta j}x_{\gamma k}x_{\gamma l}\}\notag\\
+&\frac{1}{n^4}\sum_{\alpha=1}^m\sum_{\beta=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m\ul{n}_\alpha\ul{n}_\beta\ul{n}_\gamma\ul{n}_\delta x_{\alpha i}x_{\beta j} x_{\gamma k}x_{\delta l}\label{eq-prodcov}
\end{align} is a quartic function of multinomial frequencies.

Our first and main task in this paper is computing the exact value of
the expected value of the quartic in equation \eqref{eq-prodcov}. Of
course the first order asymptotic approximation of the expected value
has been around forever. It is unclear to me if the exact value has been
computed before, but the result may very well exist in the labyrinthine
literature from around 1900 by Pearson, Sheppard, Isserlis, Edgeworth,
and others. The fact that we use discrete random variables and
frequencies of profiles (or celss) also gives our results a somewhat
nineteenth century flavor. To be honest, It also seems to me that the
usefulness of the result is rather limited compared with the effort
required to derive it.

\subsection{Multinomial Product
Moments}\label{multinomial-product-moments}

In order to compute expectations of covariances and their products we
need the moments and product moments of multinomial vectors. They have
been computed by Mosimann (\citeproc{ref-mosimann_62}{1962}), Newcomer,
Neerchal, and Morel (\citeproc{ref-newcomer_neerchal_morel_08}{2008}),
and more recently by Ouimet (\citeproc{ref-ouimet_20}{2020}), and Ouimet
(\citeproc{ref-ouimet_21}{2021}). Mosimann
(\citeproc{ref-mosimann_62}{1962}) gives a general formula for the
factorial moments, Newcomer, Neerchal, and Morel
(\citeproc{ref-newcomer_neerchal_morel_08}{2008}) give the raw moments
up to order four, Ouimet (\citeproc{ref-ouimet_20}{2020}) gives raw and
central moments up to order four, and Ouimet
(\citeproc{ref-ouimet_21}{2021}) gives raw and central moments up to
order eight. We shall derive slightly different, but equivalent,
expressions, using Kronecker deltas throughout. Our basic tool is the
moment generating function of the multinomial distribution and its
partial derivatives. We include some of the intermediate calculations,
not just the final results.

The moment generating function of the multinomial distribution with
parameters \(\pi\) and \(n\) is \begin{equation}
M(t)=\left[\sum_{\alpha=1}^M\pi_\alpha\exp(t_\alpha)\right]^n,\label{eq-mgf}
\end{equation} The raw moments of the multinomial distribution are the
partial derivatives of \(M(t)\) at \(t=0\), i.e. \begin{equation}
\mathbf{E}(\ul{n}_{\alpha_1}\cdots\ul{n}_{\alpha_r})=\mathcal{D}_{\alpha_1\cdots\alpha_r} M(0).\label{eq-m1}
\end{equation} This strongly suggests to use software for symbolic
differentation, such as Mathematica, Maple, or Maxima, but what I have
readily available does not handle an arbitrary number of variables, in
our case an arbitrary number of profiles.

It is convenient to define, for \(k=0,2,\cdots,n\), the \emph{falling
factorials} \begin{equation}
n^{(k)}:=\frac{n!}{(n-k)!}=n(n-1)\cdots(n-k+1).\label{eq-nk}
\end{equation} Note \(n^{(0)}=1\), \(n^{(1)}=n\) and \(n^{(n)}=n!\). We
will also use the shorthand \begin{equation}
\tau_\alpha(t):=\pi_\alpha\exp(t_\alpha),\label{taudef}
\end{equation} and \begin{equation}
S(t):=\sum_{\alpha=1}^m\tau_\alpha(t).\label{eq-ds}
\end{equation} Note that \(S(0)=1\) and \(\tau_\alpha(0)=\pi_\alpha\).
Also
\(\mathcal{D}_\beta\tau_\alpha(t)=\delta^{\alpha\beta}\tau_\alpha(t)\)
and \(\mathcal{D}_\alpha S(t)=\tau_\alpha(t)\). We also frequently use
some simple properties of Kronecker deltas, such as
\(\smash{\delta^{\alpha\beta}\delta^{\alpha\gamma}=\delta^{\alpha\beta\gamma}}\)
and
\(\smash{\delta^{\alpha\beta}x_\alpha=\delta^{\alpha\beta}x_\beta}\).

\subsubsection{First Order Moments}\label{first-order-moments}

The derivative of the moment generating function with respect to
\(t_\alpha\) is \begin{equation}
\mathcal{D}_\alpha M(t)=nS(t)^{n-1}\tau_\alpha(t),\label{eq-d1}
\end{equation} and thus \begin{equation}
\mathbf{E}(\ul{n}_\alpha)=\mathcal{D}_\alpha M(0)=n\pi_\alpha.\label{eq-enaa}
\end{equation}

\subsubsection{Second Order Moments}\label{second-order-moments}

Differentiating \eqref{eq-d1} with respect to \(\tau_\beta\) gives
\begin{equation}
\mathcal{D}_{\alpha\beta}M(t)=n^{(2)}S(t)^{n-2}\tau_\alpha(t)\tau_\beta(t)+nS(t)^{n-1}\delta^{\alpha\beta}\tau_\beta(t),\label{eq-d2}
\end{equation} and thus \begin{equation}
\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta)=\mathcal{D}_{\alpha\beta}M(0)=n^{(2)}\pi_\alpha\pi_\beta+n\delta^{\alpha\beta}\pi_\beta\label{eq-enab}
\end{equation}

\subsubsection{Third Order Moments}\label{third-order-moments}

Differenting \eqref{eq-d2} with respect to \(t_\gamma\) gives
\begin{subequations}
\begin{align}
\mathcal{D}_{\alpha\beta\gamma}M(t)&=n^{(3)}S(t)^{n-3}\tau_\alpha(t)\tau_\beta(t)\tau_\gamma(t)\label{eq-d3a}\\
&+n^{(2)}S(t)^{n-2}\{\delta^{\alpha\gamma}\tau_\beta(t)\tau_\gamma(t)+\delta^{\beta\gamma}\tau_\alpha(t)\tau_\gamma(t)+\delta^{\alpha\beta}\tau_\beta(t)\tau_\gamma(t)\}\label{eq-d3b}\\
&+nS(t)^{n-1}\delta^{\alpha\beta\gamma}\tau_\gamma(t)\label{eq-d3c}.
\end{align}
\end{subequations}\\
Thus \begin{equation}
\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta\ul{n}_\gamma)=\\n^{(3)}\pi_\alpha\pi_\beta\pi_\gamma+n^{(2)}\{\delta^{\alpha\gamma}\pi_\beta\pi_\gamma+\delta^{\beta\gamma}\pi_\alpha\pi_\gamma+\delta^{\alpha\beta}\pi_\beta\pi_\gamma\}+n\delta^{\alpha\beta\gamma}\pi_\gamma.\label{eq-enabg}
\end{equation}

We check \eqref{eq-enabg} against the formulas given by Newcomer,
Neerchal, and Morel (\citeproc{ref-newcomer_neerchal_morel_08}{2008}).
If \(\alpha, \beta, \gamma\) are all different, we have
\(\delta^{\alpha\gamma}=\delta^{\beta\gamma}=\delta^{\alpha\beta}=\delta^{\alpha\beta\gamma}=0\)
and thus \eqref{eq-enabg} reduces to \begin{subequations}
\begin{equation}
\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta\ul{n}_\gamma)=n^{(3)}\pi_\alpha\pi_\beta\pi_\gamma.
\label{eq-newc31}\end{equation}
If $\beta=\alpha$ and $\gamma\not=\alpha$ then $\delta^{\alpha\gamma}=\delta^{\alpha\beta}=\delta^{\alpha\beta\gamma}=0$ and $\delta^{\beta\gamma}=1$, so \eqref{eq-enabg} reduces to
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^2\ul{n}_\gamma)=n^{(3)}\pi_\alpha^2\pi_\gamma+n^{(2)}\pi_\alpha\pi_\gamma.
\label{eq-newc32}\end{equation}
If $\beta=\alpha$ and $\gamma=\alpha$ then $\delta^{\alpha\gamma}=\delta^{\beta\gamma}=\delta^{\alpha\beta}=\delta^{\alpha\beta\gamma}=1$, so \eqref{eq-enabg} reduces to
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^3)=n^{(3)}\pi_\alpha^3+3n^{(2)}\pi_\alpha^2+n\pi_\alpha.
\label{eq-newc33}\end{equation}
\end{subequations} And \eqref{eq-newc31}-\eqref{eq-newc33} are indeed
the formulas given by Newcomer, Neerchal, and Morel
(\citeproc{ref-newcomer_neerchal_morel_08}{2008}).

\subsubsection{Fourth Order Moments}\label{fourth-order-moments}

We will compute the fourth partials in steps. Differentiating
\eqref{eq-d3a} with respect to \(\tau_\delta\) gives \begin{multline}
n^{(4)}S(t)^{n-4}\tau_\alpha(t)\tau_\beta(t)\tau_\gamma(t)\tau_\delta(t)\\
+n^{(3)}S(t)^{n-3}\left\{\delta^{\gamma\delta}\tau_\alpha(t)\tau_\beta(t)\tau_\delta(t)+\delta^{\beta\delta}\tau_\alpha(t)\tau_\gamma(t)\tau_\delta(t)+\delta^{\alpha\delta}\tau_\beta(t)\tau_\gamma(t)\tau_\delta(t)\right\}.
\end{multline} For \(t=0\) this is \begin{equation}
n^{(4)}\pi_\alpha\pi_\beta\pi_\gamma\pi_\delta+n^{(3)}\{\delta^{\gamma\delta}\pi_\alpha\pi_\beta\pi_\delta+\delta^{\beta\delta}\pi_\alpha\pi_\gamma\pi_\delta+\delta^{\alpha\delta}\pi_\beta\pi_\gamma\pi_\delta\}.\label{eq-d4a}
\end{equation}

Differentiating \eqref{eq-d3b} gives \begin{multline}
n^{(3)}S(t)^{n-3}\{\delta^{\alpha\gamma}\tau_\beta(t)\tau_\gamma(t)\tau_\delta(t)+\delta^{\beta\gamma}\tau_\alpha(t)\tau_\gamma(t)\tau_\delta(t)+\delta^{\alpha\beta}\tau_\alpha(t)\tau_\gamma(t)\tau_\delta(t)\}\\
+n^{(2)}S(t)^{n-2}\{
\delta^{\alpha\gamma}\delta^{\gamma\delta}\tau_\beta(t)\tau_\delta(t)+
\delta^{\alpha\gamma}\delta^{\beta\delta}\tau_\gamma(t)\tau_\delta(t)+
\delta^{\beta\gamma}\delta^{\alpha\delta}\tau_\gamma(t)\tau_\delta(t)+\\
\delta^{\beta\gamma}\delta^{\gamma\delta}\tau_\alpha(t)\tau_\delta(t)+
\delta^{\alpha\beta}\delta^{\beta\delta}\tau_\gamma(t)\tau_\delta(t)+
\delta^{\alpha\beta}\delta^{\gamma\delta}\tau_\beta(t)\tau_\delta(t)\}.
\}
\end{multline} For \(t=0\) this is \begin{multline}
n^{(3)}\{\delta^{\alpha\gamma}\pi_\beta\pi_\gamma\pi_\delta+\delta^{\beta\gamma}\pi_\alpha\pi_\gamma\pi_\delta+\delta^{\alpha\beta}\pi_\alpha\pi_\gamma\pi_\delta\}\\
+n^{(2)}\{
\delta^{\alpha\gamma\delta}\pi_\beta\pi_\delta+
\delta^{\alpha\gamma}\delta^{\beta\delta}\pi_\gamma\pi_\delta+
\delta^{\beta\gamma}\delta^{\alpha\delta}\pi_\gamma\pi_\delta+\\
\delta^{\beta\gamma\delta}\pi_\alpha\pi_\delta+
\delta^{\alpha\beta\delta}\pi_\gamma\pi_\delta+
\delta^{\alpha\beta}\delta^{\gamma\delta}\pi_\beta\pi_\delta\}.\label{eq-d4b}
\end{multline}

Differentiating \eqref{eq-d3c} gives \begin{equation}
n^{(2)}S(t)^{n-2}\delta^{\alpha\beta\gamma}\tau_\gamma(t)\tau_\delta(t)+nS(t)^{n-1}\delta^{\alpha\beta\gamma\delta}\tau_\gamma(t).\label{eq-d4c}
\end{equation} For \(t=0\) this is \begin{equation}
n^{(2)}\delta^{\alpha\beta\gamma}\pi_\gamma\pi_\delta+n\delta^{\alpha\beta\gamma\delta}\pi_\gamma.\label{eq-d5c}
\end{equation}

Now gather the results of \eqref{eq-d4a}, \eqref{eq-d4b}, and
\eqref{eq-d5c} to get \begin{align}
\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta\ul{n}_\gamma\ul{n}_\delta)=&n^{(4)}\pi_\alpha\pi_\beta\pi_\gamma\pi_\delta+\\
&n^{(3)}\{\delta^{\gamma\delta}\pi_\alpha\pi_\beta\pi_\delta+\delta^{\beta\delta}\pi_\alpha\pi_\gamma\pi_\delta+\delta^{\alpha\delta}\pi_\beta\pi_\gamma\pi_\delta+\\
&\delta^{\alpha\gamma}\pi_\beta\pi_\gamma\pi_\delta+\delta^{\beta\gamma}\pi_\alpha\pi_\gamma\pi_\delta+\delta^{\alpha\beta}\pi_\alpha\pi_\gamma\pi_\delta\}+\\
&n^{(2)}\{\delta^{\alpha\gamma\delta}\pi_\beta\pi_\gamma+
\delta^{\alpha\gamma}\delta^{\beta\delta}\pi_\beta\pi_\gamma+
\delta^{\beta\gamma}\delta^{\alpha\delta}\pi_\alpha\pi_\gamma+\\
&\delta^{\beta\gamma\delta}\pi_\alpha\pi_\gamma+
\delta^{\alpha\beta\delta}\pi_\alpha\pi_\gamma+
\delta^{\alpha\beta}\delta^{\gamma\delta}\pi_\alpha\pi_\gamma+\delta^{\alpha\beta\gamma}\pi_\gamma\pi_\delta\}+\\
&n\delta^{\alpha\beta\gamma\delta}\pi_\gamma.
\end{align}

We check this final expression again using Newcomer, Neerchal, and Morel
(\citeproc{ref-newcomer_neerchal_morel_08}{2008}). \begin{subequations}
If $\alpha,\beta,\gamma,\delta$ are all singletons (i.e. not equal to each other or to any of the other subscripts) then the Kronecker deltas are all zero and we get
\begin{equation}
\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta\ul{n}_\gamma\ul{n}_\delta)=n^{(4)}\pi_\alpha\pi_\beta\pi_\gamma\pi_\delta.
\end{equation}
If $\alpha=\beta$ and $\gamma,\delta$ are singletons the only Kronecker delta equal to one is $\delta^{\alpha\beta}$ and we get
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^2\ul{n}_\gamma\ul{n}_\delta)=n^{(4)}\pi_\alpha^2\pi_\gamma\pi_\delta+
n^{(3)}\pi_\alpha\pi_\gamma\pi_\delta
\end{equation}
If there are only two different subscripts, with $\alpha=\beta$ and $\gamma=\delta$, then
only $\delta^{\alpha\beta}$ and $\delta^{\gamma\delta}$ are one. We get
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^2\ul{n}_\gamma^2)=n^{(4)}\pi_\alpha^2\pi_\gamma^2+
n^{(3)}(\pi_\alpha^2\pi_\gamma+\pi_\alpha^2\pi_\gamma)+n^{(2)}\pi_\alpha\pi_\gamma
\end{equation}
Again, only two different subscripts, but now $\alpha=\beta=\gamma$ and $\delta$ is a singleton. Thus 
$\delta^{\alpha\beta}=\delta^{\alpha\gamma}=\delta^{\beta\gamma}=\delta^{\alpha\beta\gamma}=1$ and the other Kronecker deltas are zero. We get
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^3\ul{n}_\delta)=n^{(4)}\pi_\alpha^3\pi_\delta+
3n^{(3)}\pi_\alpha^2\pi_\delta+n^{(2)}\pi_\alpha\pi_\delta
\end{equation}
Finally, if $\alpha=\beta=\gamma=\delta$ then all Kronecker deltas are one and we get
\begin{equation}
\mathbf{E}(\ul{n}_\alpha^4)=n^{(4)}\pi_\alpha^4+
6n^{(3)}\pi_\alpha^3+7n^{(2)}\pi_\alpha^2+n\pi_\alpha
\end{equation}
\end{subequations} These are indeed the same results as in Newcomer,
Neerchal, and Morel (\citeproc{ref-newcomer_neerchal_morel_08}{2008}).

\subsection{Covariance of Covariances}\label{covariance-of-covariances}

We now switch from computing moments of the multinomial random variable
indicating the profile to the random variable that takes the profiles
themselves as its values, with the same multinomial probabilities. We
will compute the covariance of two covariances, i.e.~the expected value
of the product of two covariances minus the product of the expected
values of the covariances.

From equation \eqref{eq-covfreq} we have \begin{equation}
\mathbf{E}(\ul{c}_{ij})=\frac{1}{n}\sum_{\nu=1}^m\mathbf{E}(\ul{n}_\nu) x_{\nu i}x_{\nu j}-\frac{1}{n^2}\sum_{\alpha=1}^m\sum_{\beta=1}^m\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta) x_{\alpha i}x_{\beta j}.\label{eq-ecov}
\end{equation} Using equations \eqref{eq-enaa} and \eqref{eq-enab} we
get \begin{equation}
\mathbf{E}(\ul{c}_{ij})=\frac{n-1}{n}(\mu_{ij}-\mu_i\mu_j)=\frac{n-1}{n}\sigma_{ij}.
\label{eq-ecov2}
\end{equation} This is, of course, an exact result, not an
approximation. It is the well-known result that an unbiased estimate of
the covariance is the sample covariance, multiplied by \(n/(n-1)\).

Next we compute the covariance of covariances. From equation
\eqref{eq-prodcov} we have \begin{align}
\mathbf{E}(\ul{c}_{ij}\ul{c}_{kl})=&\frac{1}{n^2}\sum_{\nu=1}^m\sum_{\eta=1}^m\mathbf{E}(\ul{n}_\nu \ul{n}_\eta) x_{\nu i}x_{\nu j}x_{\eta k}x_{\eta l}\\
-&\frac{1}{n^3}\sum_{\nu=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m\mathbf{E}(\ul{n}_\nu \ul{n}_\gamma\ul{n}_\delta)x_{\nu i}x_{\nu j}x_{\gamma k}x_{\delta l}\\
-&\frac{1}{n^3}\sum_{\eta=1}^m\sum_{\alpha=1}^m\sum_{\beta=1}^m\mathbf{E}(\ul{n}_\eta \ul{n}_\alpha\ul{n}_\beta)x_{\eta k}x_{\eta l}x_{\alpha i}x_{\beta j}\\
+&\frac{1}{n^4}\sum_{\alpha=1}^m\sum_{\beta=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m\mathbf{E}(\ul{n}_\alpha\ul{n}_\beta\ul{n}_\alpha\ul{n}_\beta) x_{\alpha i}x_{\beta j} x_{\gamma k}x_{\delta l}
\end{align}

From here we again proceed in steps. The first term of the right hand
side of the equation above is \[
\sum_{\nu=1}^m\sum_{\eta=1}^m\mathbf{E}(\ul{n}_\nu \ul{n}_\eta) x_{\nu i}x_{\nu j}x_{\eta k}x_{\eta l}=\sum_{\nu=1}^m\sum_{\eta=1}^m(n^{(2)}\pi_\nu\pi_\eta+n\delta^{\nu\eta}\pi_\nu)x_{\nu i}x_{\nu j}x_{\eta k}x_{\eta l}=n^{(2)}\mu_{ij}\mu_{kl}+n\mu_{ijkl}
\] For the second term of \ldots{} we need \begin{align}
\mathbf{E}(\ul{n}_\nu\ul{n}_\gamma\ul{n}_\delta)&=
n^{(3)}\pi_\nu\pi_\gamma\pi_\delta+\delta^{\nu\gamma\delta}(3n^{(2)}\pi_\nu^2+n\pi_\nu)\\
&+\delta^{\nu\gamma}(1-\delta^{\nu\delta})(1-\delta^{\gamma\delta})n^{(2)}\pi_\nu\pi_\delta\\&+\delta^{\gamma\delta}(1-\delta^{\nu\delta})(1-\delta^{\nu\gamma})n^{(2)}\pi_\nu\pi_\gamma\\&+\delta^{\nu\delta}(1-\delta^{\nu\gamma})(1-\delta^{\gamma\delta})n^{(2)}\pi_\gamma\pi_\delta
\end{align}

\[
n^{(3)}\mu_{ij}\mu_k\mu_l+3n^{(2)}\sum_{\nu=1}^m\pi_\nu^2x_{\nu i}x_{\nu j}x_{\nu k}x_{\nu l}+n\mu_{ijkl}
\] \[
\delta^{\nu\gamma}(1-\delta^{\nu\delta})(1-\delta^{\gamma\delta})=\delta^{\nu\gamma}-\delta^{\nu\gamma\delta}\]
\[
n^{(2)}\sum_{\nu=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m(\delta^{\nu\gamma}-\delta^{\nu\gamma\delta})\pi_\nu\pi_\delta x_{\nu i}x_{\nu j}x_{\gamma k}x_{\delta l}=n^{(2)}\{\mu_{ijk}\mu_l-\sum_{\nu=1}^m\pi_\nu^2x_{\nu i}x_{\nu j}x_{\nu k}x_{\nu l}\}
\] \[
n^{(2)}\sum_{\nu=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m(\delta^{\gamma\delta}-\delta^{\nu\gamma\delta})\pi_\nu\pi_\gamma x_{\nu i}x_{\nu j}x_{\gamma k}x_{\delta l}=n^{(2)}\{\mu_{ij}\mu_{kl}-\sum_{\nu=1}^m\pi_\nu^2x_{\nu i}x_{\nu j}x_{\nu k}x_{\nu l}\}
\] \[
n^{(2)}\sum_{\nu=1}^m\sum_{\gamma=1}^m\sum_{\delta=1}^m(\delta^{\nu\delta}-\delta^{\nu\gamma\delta})\pi_\gamma\pi_\delta x_{\nu i}x_{\nu j}x_{\gamma k}x_{\delta l}=n^{(2)}\{\mu_{ijl}\mu_k-\sum_{\nu=1}^m\pi_\nu^2x_{\nu i}x_{\nu j}x_{\nu k}x_{\nu l}\}
\]

\subsection{Asymptotics}\label{asymptotics}

From the result of the previous section \[
\text{COV}(\ul{c}_{ij},\ul{c}_{kl})=n^{-1}(\sigma_{ijkl}-\sigma_{ij}\sigma_{kl})+o(n^{-1})
\] We now apply .. to computing the covariance of correlations, defined
as \[
\ul{r}_{ij}=\ul{c}_{ij}^{\ }\ul{c}_{ii}^{-\frac12}\ul{c}_{jj}^{-\frac12}
\] If we define \[
\ul{z}_{ij}:=n^\frac12(\ul{c}_{ij}-\sigma_{ij})
\] we have \[
\ul{r}_{ij}=(\sigma_{ij}+n^{-\frac12}\ul{z}_{ij})(\sigma_{ii}+n^{-\frac12}\ul{z}_{ii})^{-\frac12}(\sigma_{jj}+n^{-\frac12}\ul{z}_{jj})^{-\frac12},
\] which implies \begin{equation}
\ul{r}_{ij}=\rho_{ij}+n^{-\frac12}\rho_{ij}\left\{\frac{\ul{z}_{ij}}{\sigma_{ij}}-\frac12\frac{\ul{z}_{ii}}{\sigma_{ii}}-\frac12\frac{\ul{z}_{jj}}{\sigma_{jj}}\right\}+o_p(n^{-\frac12}).\label{eq-rexp}
\end{equation} Multiplying \eqref{eq-rexp} for \(\ul{r}_{ij}\) and
\(\ul{r}_{kl}\), and simplifying gives \begin{multline}
n\text{COV}(\ul{r}_{ij},\ul{r}_{kl})=
\rho_{ij}\rho_{kl}\left\{\frac{\sigma_{ijkl}}{\sigma_{ij}\sigma_{kl}}
-\frac12\left(\frac{\sigma_{ijkk}}{\sigma_{ij}\sigma_{kk}}
+\frac{\sigma_{ijll}}{\sigma_{ij}\sigma_{ll}}
+\frac{\sigma_{iikl}}{\sigma_{ii}\sigma_{kl}}
+\frac{\sigma_{jjkl}}{\sigma_{jj}\sigma_{kl}}\right)\right.\\
\left.+\frac14\left(\frac{\sigma_{iikk}}{\sigma_{ii}\sigma_{kk}}
+\frac{\sigma_{iill}}{\sigma_{ii}\sigma_{ll}}+
+\frac{\sigma_{jjkk}}{\sigma_{jj}\sigma_{kk}}+
+\frac{\sigma_{jjll}}{\sigma_{jj}\sigma_{ll}}\right)\right\}.\label{eq-covr}
\end{multline} A further simplification is possible by defining
\begin{equation}
\rho_{ijkl}:=\frac{\sigma_{ijkl}}{\sqrt{\sigma_{ii}\sigma_{jj}\sigma_{kk}\sigma_{ll}}}.
\label{eq-normcor}
\end{equation} Equation \eqref{eq-covr} becomes \begin{align}
n\text{COV}(\ul{r}_{ij},\ul{r}_{kl})=\rho_{ijkl}&-\frac12\rho_{kl}(\rho_{ijkk}+\rho_{ijll})-\frac12
\rho_{ij}(\rho_{iikl}+\rho_{jjkl})\notag\\&+\frac14\rho_{ij}\rho_{kl}(\rho_{iikk}+\rho_{iill}+\rho_{jjkk}+\rho_{jjll}).\label{eq-ihsh}
\end{align} Equation \eqref{eq-ihsh} has been rediscovered every 33
years by successive generations of statisticians (Isserlis
(\citeproc{ref-isserlis_16}{1916}), Hsu (\citeproc{ref-hsu_49}{1949}),
Steiger and Hakstian (\citeproc{ref-steiger_hakstian_82}{1982})).

\[
\begin{bmatrix}
\frac{\partial \rho_{ij}}{\partial \sigma_{ij}}\\
\frac{\partial \rho_{ij}}{\partial \sigma_{ii}}\\
\frac{\partial \rho_{ij}}{\partial \sigma_{jj}}
\end{bmatrix}=
\begin{bmatrix}
\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac12}\\
-\frac12 \sigma_{ij}\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac12}\\
-\frac12 \sigma_{ij}\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac32}
\end{bmatrix}
\] \[
\begin{bmatrix}
\frac{\partial^2\rho_{ij}}{\partial \sigma_{ij}\partial \sigma_{ij}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{ij}\partial \sigma_{ii}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{ij}\partial \sigma_{jj}}\\
\frac{\partial^2\rho_{ij}}{\partial \sigma_{ii}\partial \sigma_{ij}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{ii}\partial \sigma_{ii}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{ii}\partial \sigma_{jj}}\\
\frac{\partial^2\rho_{ij}}{\partial \sigma_{jj}\partial \sigma_{ij}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{jj}\partial \sigma_{ii}}&\frac{\partial^2\rho_{ij}}{\partial \sigma_{jj}\partial \sigma_{jj}}
\end{bmatrix}=
\begin{bmatrix}
0&-\frac12\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac12}&-\frac12\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac32}\\
-\frac12\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac12}&\frac34\sigma_{ij}\sigma_{ii}^{-\frac52}\sigma_{jj}^{-\frac12}&
\frac14\sigma_{ij}\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac32}\\
-\frac12\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac32}&\frac14\sigma_{ij}\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac32}&\frac34\sigma_{ij}\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac52}
\end{bmatrix}
\] \[
\begin{bmatrix}
\sigma_{iijj}-\sigma_{ij}\sigma_{ij}&\sigma_{iiij}-\sigma_{ij}\sigma_{ii}&\sigma_{ijjj}-\sigma_{ij}\sigma_{jj}\\
\sigma_{iiij}-\sigma_{ii}\sigma_{ij}&\sigma_{iiii}-\sigma_{ii}\sigma_{ii}&\sigma_{iijj}-\sigma_{ii}\sigma_{jj}\\
\sigma_{ijjj}-\sigma_{jj}\sigma_{ij}&\sigma_{iijj}-\sigma_{jj}\sigma_{ii}&\sigma_{jjjj}-\sigma_{jj}\sigma_{jj}
\end{bmatrix}
\] \[
n(\mathbf{E}(\ul{r}_{ij})-\rho_{ij})=
-\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac12}(\sigma_{iiij}-\sigma_{ij}\sigma_{ii})
-\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac32}(\sigma_{ijjj}-\sigma_{jj}\sigma_{ij})\\
+\frac12\sigma_{ij}\sigma_{ii}^{-\frac32}\sigma_{jj}^{-\frac32}(\sigma_{iijj}-\sigma_{jj}\sigma_{ii})
+\frac34\sigma_{ij}\sigma_{ii}^{-\frac52}\sigma_{jj}^{-\frac12}(\sigma_{iiii}-\sigma_{ii}\sigma_{ii})
+\frac34\sigma_{ij}\sigma_{ii}^{-\frac12}\sigma_{jj}^{-\frac52}(\sigma_{jjjj}-\sigma_{jj}\sigma_{jj})
\]

\sectionbreak

\section{Discussion}\label{discussion}

Our formulas have been derived in a discrete probability framework, so
someone may ask about situations in which the data are realizations of
continuous random variables. But even if they are, or more precisely if
they are modeled to be, the data themselves are always discrete.
Calculations with continuous random variables are a means of
approximating more complicated calculations with discrete variables.
Continuity is always part of the model, and always unobserved. There is
no such thing as continuous data (Holland
(\citeproc{ref-holland_79}{1979}), Gifi
(\citeproc{ref-gifi_B_90}{1990})).

The formulas we have derived are valid, no matter how we discretize the
data. Of course the actual values of the moments and product moments
will depend on the discretization, but the formulas themselves are
independent of the fineness or crudeness of the discretization. As a
limiting case, our discretization can be so fine that there is at most
one observation in each cell. Or, more precisely, observations are in
the same cell only if they are equal. In that limiting case the
realizations \(\ul{n}_\alpha\) are either zero or one, and the formulas
reduce to the more familiar formulas for the case of repeated
independent trials.

\sectionbreak

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-gifi_B_90}
Gifi, A. 1990. \emph{Nonlinear Multivariate Analysis}. New York, N.Y.:
Wiley.

\bibitem[\citeproctext]{ref-hemelrijk_66}
Hemelrijk, J. 1966. {``{Underlining Random Variables}.''}
\emph{Statistica Neerlandica} 20: 1--7.

\bibitem[\citeproctext]{ref-holland_79}
Holland, P. W. 1979. {``{The Tyranny of Continuous Models in a World of
Discrete Data}.''} \emph{IHS-Journal} 3: 29--42.

\bibitem[\citeproctext]{ref-hsu_49}
Hsu, P. L. 1949. {``The Limiting Distribution of Functions of Sample
Means and Application to Testing Hypotheses.''} In \emph{Proceedings of
the First Berkeley Symposium on Mathematical Statistics and
Probability}, edited by Neyman J, 359--401. University of California
Press.

\bibitem[\citeproctext]{ref-isserlis_16}
Isserlis, L. 1916. {``On Certain Probable Errors and Correlation
Coefficients of Multiple Frequency Distributions with Skew
Regression.''} \emph{Biometrika} 11 (3): 185--90.

\bibitem[\citeproctext]{ref-mann_wald_43}
Mann, H. B., and A. Wald. 1943. {``{On Stochastic Limit and Order
Relationships}.''} \emph{Annals of Mathematical Statistics} 14: 217--26.

\bibitem[\citeproctext]{ref-mosimann_62}
Mosimann, J. E. 1962. {``On the Compound Multinomial Distribution, the
Multivariate Î² -Distribution, and Correlations Among Proportions.''}
\emph{Biometrika} 49: 65--82.

\bibitem[\citeproctext]{ref-newcomer_neerchal_morel_08}
Newcomer, J. T., N. K. Neerchal, and J. G. Morel. 2008. {``Computation
of Higher Order Moments from Two Multinomial Overdispersion Likelihood
Models.''}
\url{https://www.semanticscholar.org/paper/Computation-of-Higher-Order-Moments-from-Two-Models-Ewcomer/91d6e0b024b63239e85a12e11052e0d5717487e4}.

\bibitem[\citeproctext]{ref-ouimet_20}
Ouimet, F. 2020. {``Explicit Formulas for the Joint Third and Fourth
Central Moments of the Multinomial Distribution.''}
\url{https://arxiv.org/pdf/2006.09059}.

\bibitem[\citeproctext]{ref-ouimet_21}
---------. 2021. {``General Formulas for the Central and Non-Central
Moments of the Multinomial Distribution.''} \emph{Stats} 4: 18--27.

\bibitem[\citeproctext]{ref-r_core_team_24}
R Core Team. 2024. \emph{R: A Language and Environment for Statistical
Computing}. {Vienna, Austria}: R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\bibitem[\citeproctext]{ref-steiger_hakstian_82}
Steiger, J. H., and A. R. Hakstian. 1982. {``{The Asymptotic
Distribution of Elements of a Correlation Matrix: Theory and
Application}.''} \emph{British Journal of Mathematical and Statistical
Psychology} 35: 208--15.

\end{CSLReferences}




\end{document}
